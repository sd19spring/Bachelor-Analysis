{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Write-Up and Reflection: The Bachelor üåπ\n",
    "By:  Kristin Aoki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I decided to use Twitter as my data source and analyzed my findings using word frequency. The data I collected was from the timeline of ABC's hit reality TV show, _The Bachelor_. From this project, I hoped to learn how to use APIs and figure out the most associated words with this season of _The Bachelor_. As a devoted viewer of the show, I hear many catchphrases in the show and share many of the same reactions as people across the country. By determining the most associated words with this season of _The Bachelor_, I can determine to confirm my beliefs of the most stated words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accomplish the goal of my project I had to create a Twitter app and implement the keys into my code. To collect my data I decided to look at two different Twitter timelines and #TheBachelor. One timeline was the official Bachelor Twitter page (@BachelorABC), and the other was a popular fan page (@bachelorburnbk). I decide to use these three sections of Twitter because it would provide me well-represented data. Once I mined them from Twitter, I saved the data from timelines to a data frame, then to a .txt file. I saved the data from the hashtag to a .csv file, then copied and pasted the information into a .txt file, so my word frequency code could read the tweets. To generate the word cloud, I used an online application were I input my.txt files with all the words and made sure that the word frequencies matched the values my code generated.\n",
    "\n",
    "I decided to put the tweets from the user timelines into a data frame because it was the easiest way to break up the tweets into individual lines instead of a long list of hundreds of tweets. Placing the tweets into a data frame also made it easier to check that I mining the correct information from Twitter. I decided to place the tweets from the hashtag into a .csv file at first because it was simpler than the data frame for collecting from a hashtag. I could have tried the same method for the timelines, but I was afraid that it would mess up the data that I already collected. \n",
    "\n",
    "The final presentation of my output is in a word bubble, but my code does not generate the word bubble. I decided to use an online application to generate the word bubble because it makes the output look more interesting than three sets of lists. However, two of the most frequents words are not represented in the word bubbles because they were emojis, a heart and clapping hands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of my data included the words that I thought would be included, as well as words that I did not think,  would be included. I knew that Colton (the name of the bachelor), fence, love, and names of the contestants would be some of the most frequent words. However, I did not expect Chris Harrison (host/producer), emojis, values, and beliefs. One word that I expected but did not appear, was virgin/virginity since it is one of the hottest topics of the season. \n",
    "\n",
    "The following list are the most frequent words that are also represented in the word bubbles.\n",
    "- ['colton', 'bachelorabc', 'womentellall', 'tonight', 'chrisbharrison', 'fence', 'women', 'bachelor', 'wow', 'will', 'what', 'most', 'episode', 'watching', 'tell', 'moment', 'bachelornation', 'about', 'üëèüèΩ', 'ready', 'out', 'night', 'fantasy', 'see', 'no', 'his', 'ever', 'with', 'watch', 'waiting', 'suites', 'right', 'one', 'now', 'history', 'here', 'have', 'from', \"colton's\", '‚ù§Ô∏è', 'y‚Äôall', 'your', 'wait', 'sweet', 'over', 'omg', 'next', 'miss', 'love', 'live']\n",
    "- ['colton', 'harrison', 'chris', 'will', 'week', 'next', 'see', 'out', 'us', 'they', 'have', 'going', 'fence', 'womentellall', 'time', 'sure', 'last', 'from', 'continued', 'with', 'where', \"we're\", 'vlogs', 'very', 'top', 'someone', 'show', 'run', 'minutes', 'left', 'know', 'jump', 'he', 'hannah', 'find', 'everyone', 'episode', 'do', 'cry', 'could', 'cassie', 'can', 'away', 'after', 'would', 'what', 'went', 'way', 'want', 'through']\n",
    "- ['colton', 'with', 'about', 'cassie', 'caelynn', 'family', 'ready', 'hannah', 'she', 'love', 'fence', 'her', 'week', 'he', 'dad', 'next', 'hand', 'daughters', 'conversations', 'fantasysuitez', 'members', 'meaningful', 'n\\\\n*colton', \"marriage\\\\xe2\\\\x80\\\\xa6'\\t\\t\\t\\t\\t\\t\\t\\t\", 'bachelorabc', 'like', 'end', 'they', 'can', 'women', 'will', 'make', 'one', 'bachelor', 'from', 'out', 'who', 'own', 'stop', 'values', 'clearly', 'beliefs', 'episode', 'decisions', 'your', 'independent', 'tayshia', 'see', 'permission', 'marry']\n",
    "\n",
    "All of the lists have many similarities and so very stark differences. All of them present a well-rounded view of people's words pertaining to _The Bachelor_.\n",
    "\n",
    "<img src=\"wordcloudBachelorABC.jpg\" width=\"400\"> \n",
    "<img src=\"wordcloudbachelorburnbk.jpg\" width=\"400\">\n",
    "<img src=\"wordcloudthebachelor.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring the various data sources provided, I determined Twitter was the most applicable source for my project. None of the other sources would provide data of people's reactions to _The Bachelor_. Reddit was a possible option but it was really confusing and many of the sub-threads did not relate to the information that I wanted to collect.\n",
    "\n",
    "Using Twitter as my data source produced results that I expected, but I had to filter out a lot of words because they were not words or did not pertain to the topic (e.i. this, i'm, no, etc.). A limitation of using twitter is that you can only collect so much data. All the data that I collected was of reactions from the latest episodes of _The Bachelor_. Even though I gave my code parameters to mine tweets from the start of the show (January 7th, 2019), Twitter would not allow me to go back that far and my computer could not store all of that data. Another limitation is that not all tweets related to _The Bachelor_ are posted to the @BachelorABC and @bachelorburnbk timeline or contain #TheBachelor. As a result, I am not able to collect all the data to accurately represent the words most associated with _The Bachelor_.\n",
    "\n",
    "I could have had more accurate data if I had collected more tweets with different hashtags or from different timelines, e.i. the bachelor's or contestants' timelines. Despite these additional options, I am very satisfied with the data I produced because represent a lot of the words that I and my friends who watch _The Bachelor_ think of when we hear _The Bachelor_ mentioned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall my project went really well. The project was well scoped for me because it pushed my coding skills and forced me to learn new things on my own. When I first began the project I thought it was going be too much to handle, but after researching how to use an AP the project became easier. This project taught me how to produce cleaner code. During the project I was writing lots of different functions while reviewing my code I realized that a lot of the code could be simplified into more universal functions, eliminating serval lines of code. One thing I did poorly for this project was time management because I was intimidated by the topic and processes laid out for us. I put off starting/collecting data for the project because I did not understand how to collect the data for my topic. However, once I forced myself to sit down and learn about APIs and mining from Twitter I began to feel more relaxed about the project. I was so excited when I generated my first tweet from Twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
